\paragraph{Why using non-linear activation functions. }

A non-linear activation function is needed between convolution layers (i.e. hidden layers) in order to get higher layer feature representation. In fact, in a single-layer neural network, we can think of the outputs $o$ as being the result of a given activation function $f$ applied to the dot product between the inputs $x$ and the weights $W$ of the layer. $o = f(W . x)$
What we can observe in this case is that if we are using linear activation (thus, $f(x) = x$), then a two-layer network described by :
$ o_1 = W_1 . x_1 $
$ o_2 = W_2 . o_1 = W_2 . W_1 . x $
can be replaced by a single-layer network with parameters $W_{12} = W_2 . W_1$, so stacking more than one linear layer is only going to improve the complexity of the network instead of its expressivity. This is not true when using a non-linear activation function.

\subsubsection{Sigmoid}

0..1

\subsubsection{Hyperbolic Tangent}

-1..1

\subsubsection{Threshold}


\subsubsection{ReLU}

This activation function, denoted as Rectified Linear Units, introduces a non linearity into the neural net. 

  \begin{center}
  \begin{tikzpicture}
    \pgftext[base, x=   0cm, y= 0cm] {ReLU}; 
    \pgftext[base, x=  -3cm, y= 0cm] {$\mathbf{x} = \begin{bmatrix}x_1\\ \vdots \\x_D\end{bmatrix}$}; 
    \pgftext[base, x=   3cm, y= 0cm] {$\mathbf{y} = \begin{bmatrix}y_1\\ \vdots \\y_D\end{bmatrix}$}; 
    %\pgftext[base, x=   0cm, y=-1cm] {$\mathbf{w}$}; 
    \draw[black,thick] (-1.6cm,-0.3cm) rectangle (1.6cm,0.5cm); 
    \draw[->] ( -2cm, 0.1cm) -- (-1.6cm, 0.1cm);
    \draw[->] (1.6cm, 0.1cm) -- (   2cm, 0.1cm);
    %\draw[->] (  0cm,-0.6cm) -- (   0cm,-0.3cm);
  \end{tikzpicture}
  \end{center}

\subsubsection{PReLU}

http://arxiv.org/pdf/1502.01852v1.pdf




\noindent
Forward:
\begin{align}
 y_k = \max(x_k, 0) \nonumber
\end{align}

\noindent
Backward:
\begin{align}
 \frac{dy_{k_2}}{dx_{k_1}} &= \delta_{x_{k_1}>0} \hspace{0.3cm} \text{if} \hspace{0.3cm} k_1=k_2 \nonumber \\
                           &= 0 \hspace{0.3cm} \text{otherwise} \nonumber
\end{align}





